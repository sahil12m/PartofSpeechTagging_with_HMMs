{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Vocabulary Creation (20 points)\n",
    "#### The first task is to create a vocabulary using the training data. In HMM, one important problem when creating the vocabulary is to handle unknown words. One simple solution is to replace rare words whose occurrences are less than a threshold (e.g. 3) with a special token ‘< unk >’. Task. Generate a vocabulary from the training data stored in the ”train” file and save this vocabulary as ”vocab.txt.” The format of the ”vocab.txt” file should adhere to the following specifications: Each line should consist of a word type, its index within the vocabulary, and its frequency of occurrence, with these elements separated by the tab symbol (\\t ). The initial line should feature the special token ”< unk >,” followed by subsequent lines sorted in descending order of occurrence frequency. Please take into account that you are only allowed to use the training data to construct this vocabulary, and you must refrain from using the development and test data.\n",
    "##### For instance:\n",
    "##### < unk > 0 2000\n",
    "##### word1 1 20000\n",
    "##### word2 2 10000\n",
    "##### Additionally, kindly provide answers to the following questions:\n",
    "##### What threshold value did you choose for identifying unknown words for replacement?\n",
    "##### What is the overall size of your vocabulary, and how many times does the special token ”< unk >” occur following the replacement process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANS 1) The threshold value is 2 for replacement. 2) Overall size of vocabulary is 22689 and the token \"<unk>\" appears 19756 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 2\n",
      "Total size of vocabulary: 22689\n",
      "Count of <unk> tag in vocab: 19756\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the training data from the JSON file\n",
    "with open('data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Initialize the vocabulary with '<unk>'\n",
    "vocab = {'<unk>': 0}\n",
    "threshold = 2\n",
    "\n",
    "# Iterate through the sentences in the training data\n",
    "for item in train_data:\n",
    "    sentence = item['sentence']\n",
    "    for word in sentence:\n",
    "        if word.isnumeric():\n",
    "            word = '<num>'\n",
    "        if word not in vocab.keys():\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "\n",
    "# Remove words with frequency less than threshold and make them unknown\n",
    "for key in list(vocab.keys())[1:]:\n",
    "    if vocab[key] < threshold:\n",
    "        vocab['<unk>'] += vocab[key]\n",
    "        del vocab[key]\n",
    "\n",
    "# Sort the vocabulary by frequency\n",
    "vocab = dict(sorted(vocab.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Ensure that '<unk>' is at the top\n",
    "if '<unk>' in vocab:\n",
    "    # Create a new dictionary with '<unk>' as the first item\n",
    "    new_vocab = {'<unk>': vocab['<unk>']}\n",
    "    # Add the rest of the vocabulary items\n",
    "    for word, freq in vocab.items():\n",
    "        if word != '<unk>':\n",
    "            new_vocab[word] = freq\n",
    "    vocab = new_vocab\n",
    "\n",
    "# Print statistics and save to vocab.txt\n",
    "print('Threshold:', threshold)\n",
    "print('Total size of vocabulary:', len(vocab))\n",
    "print('Count of <unk> tag in vocab:', vocab['<unk>'])\n",
    "\n",
    "with open('vocab.txt', 'w') as f:\n",
    "    for index, (key, count) in enumerate(vocab.items()):\n",
    "        f.write(f'{key}\\t{index}\\t{count}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation of the above code:\n",
    "\n",
    "##### Loading Data: The code starts by loading training data from a JSON file.\n",
    "\n",
    "##### Initializing Vocabulary: It initializes a vocabulary dictionary with a single entry, '<unk>', for handling unknown words.\n",
    "\n",
    "##### Setting Threshold: A threshold value of 2 is defined to identify rare words.\n",
    "\n",
    "##### Processing Sentences: The code processes each sentence in the training data one by one.\n",
    "\n",
    "##### Processing Words: For each sentence, it processes individual words. Numeric words are replaced with '<num>', and new words are added to the vocabulary.\n",
    "\n",
    "##### Removing Rare Words: Words with counts below the threshold are replaced with '<unk>' and removed from the vocabulary.\n",
    "\n",
    "##### Sorting Vocabulary: The vocabulary is sorted by word frequency in descending order.\n",
    "\n",
    "##### Saving Vocabulary: The final vocabulary is saved to a text file ('vocab.txt') with word, index, and frequency details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 2: Model Learning (20 points)\n",
    "\n",
    "##### The second task is to learn an HMM from the training data. Remember that the solution of the emission and transition parameters in HMM are in the following formulation:\n",
    "##### t(s′|s) = count(s→s′)/count(s)\n",
    "##### e(x|s) = count(s→x)/count(s)\n",
    "##### π(s) = count(null→s)/count(num sentences)\n",
    "\n",
    "##### t(·|·) is the transition parameter.\n",
    "##### e(·|·) is the emission parameter.\n",
    "##### π(·) is the initial state (The sentence begins with this state); also called as prior probabilities.\n",
    "\n",
    "##### Task. Learning a model using the training data in the file train and output the learned model into a model file in json format, named hmm.json. The model file should contains two dictionaries for the emission and transition parameters, respectively. The first dictionary, named transition, contains items with pairs of (s,s′) as key and t(s′|s) as value. The second dictionary, named emission, contains items with pairs of (s,x) as key and e(x|s) as value. Additionally, kindly provide answers to the following questions:\n",
    "\n",
    "##### How many transition and emission parameters in your HMM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans2) the number of transition parameters is 1392 and the number of emission paramters is 29799."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transition parameters: 1392\n",
      "Number of emission parameters: 29799\n"
     ]
    }
   ],
   "source": [
    "# Load the training data from the JSON file\n",
    "with open('data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "\n",
    "transition_probabilities = {}\n",
    "emission_probabilities = {}\n",
    "state_list = []\n",
    "label_cnt = {}\n",
    "\n",
    "# Learn transition and emission parameters from the training data\n",
    "for item in train_data:\n",
    "    sentence = item['sentence']\n",
    "    labels = item['labels']\n",
    "    prev_state = 'Initial_state'\n",
    "\n",
    "    for word, label in zip(sentence, labels):\n",
    "        # Preprocess the word (e.g., handle numbers and unknown words)\n",
    "        label_cnt[label] = label_cnt.get(label, 0) + 1\n",
    "        if word.isnumeric():\n",
    "            word = '<num>'\n",
    "        if word not in vocab:\n",
    "            word = '<unk>'\n",
    "        if label not in state_list:\n",
    "            state_list.append(label)\n",
    "        # Update transition parameters\n",
    "        # if prev_state is not None:\n",
    "        if str((prev_state, label)) not in transition_probabilities.keys():\n",
    "            transition_probabilities[str((prev_state, label))] = 1\n",
    "        else:\n",
    "            transition_probabilities[str((prev_state, label))] += 1\n",
    "        if str((label, word)) not in emission_probabilities.keys():\n",
    "            emission_probabilities[str((label, word))] = 1\n",
    "        else:\n",
    "            emission_probabilities[str((label, word))] += 1\n",
    "        prev_state = label\n",
    "\n",
    "total_counts_trans = {}\n",
    "# Calculate the total counts\n",
    "for key, value in transition_probabilities.items():\n",
    "    key_tuple = eval(key)  # Convert the string key back to a tuple\n",
    "    previous, current = key_tuple  # Unpack the tuple key\n",
    "    if previous not in total_counts_trans:\n",
    "        total_counts_trans[previous] = 0\n",
    "    if previous != 'Initial_state':\n",
    "        total_counts_trans[previous] = label_cnt[previous]\n",
    "        transition_probabilities[key] = value / total_counts_trans[previous]\n",
    "\n",
    "\n",
    "total_counts_em = {}\n",
    "for key, value in emission_probabilities.items():\n",
    "    key_tuple = eval(key)  # Convert the string key back to a tuple\n",
    "    label, word = key_tuple  # Unpack the tuple key\n",
    "    if label not in total_counts_em:\n",
    "        total_counts_em[label] = 0\n",
    "    \n",
    "    total_counts_em[label] = label_cnt[label]\n",
    "    emission_probabilities[key] = value / total_counts_em[label]\n",
    "\n",
    "# Create the HMM model dictionary\n",
    "hmm_model = {\n",
    "    'transition': transition_probabilities,\n",
    "    'emission': emission_probabilities,\n",
    "}\n",
    "\n",
    "# Save the HMM model to a JSON file\n",
    "with open('hmm.json', 'w') as f:\n",
    "    json.dump(hmm_model, f, indent=4)\n",
    "\n",
    "num_transition_params = len(transition_probabilities.keys())\n",
    "num_emission_params = len(emission_probabilities.keys())\n",
    "\n",
    "# Print the number of parameters\n",
    "print(f'Number of transition parameters: {num_transition_params}')\n",
    "print(f'Number of emission parameters: {num_emission_params}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation of the above code:\n",
    "\n",
    "##### Loading Training Data: The code begins by opening and reading a JSON file named 'train.json', which contains training data. This data likely consists of sentences and their corresponding part-of-speech labels.\n",
    "\n",
    "##### Initialization: Several variables are initialized:\n",
    "##### transition_probabilities and emission_probabilities are dictionaries that will store the learned transition and emission probabilities for the Hidden Markov Model (HMM).\n",
    "##### state_list will keep track of all unique part-of-speech labels observed in the training data.\n",
    "##### label_cnt is a dictionary that will count the occurrences of each part-of-speech label.\n",
    "##### Learning Parameters: The code iterates through each item in the training data. Each item typically represents a sentence and its associated part-of-speech labels.\n",
    "\n",
    "##### Preprocessing Words and Labels: For each word and its corresponding label in the sentence, the code preprocesses the word by checking if it's numeric or not in the same way as Task 1. It also tracks the unique labels observed and counts their occurrences.\n",
    "\n",
    "##### Learning Transition Probabilities: The code calculates the transition probabilities between part-of-speech labels. It checks if a transition from the previous label (prev_state) to the current label (label) has been observed. If not, it initializes the count to 1; otherwise, it increments the count. The transition probabilities are based on how often transitions occur in the training data.\n",
    "\n",
    "##### Learning Emission Probabilities: Similar to transition probabilities, the code calculates emission probabilities between part-of-speech labels and words. It checks if an emission from a label to a word has been observed. If not, it initializes the count to 1; otherwise, it increments the count. Emission probabilities reflect how often a word is emitted from a particular label.\n",
    "\n",
    "##### Normalizing Probabilities: After counting the occurrences, the code calculates the normalized probabilities. It divides the counts by the total occurrences of labels to obtain probabilities in the range [0, 1].\n",
    "\n",
    "##### Creating the HMM Model: The code organizes the learned transition and emission probabilities into an HMM model dictionary named hmm_model.\n",
    "\n",
    "##### Saving to JSON: The HMM model dictionary is saved to a JSON file named 'hmm.json' for later use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 3: Greedy Decoding with HMM (30 points)\n",
    "##### The third task is to implement the greedy decoding algorithm with HMM.\n",
    "##### Task. Implementing the greedy decoding algorithm and evaluate it on the development data. Predict the part-of-speech tags of the sentences in the test data and output the predictions in a file named greedy.json, in the same format of training data. Additionally, kindly provide answers to the following questions:\n",
    "##### What is the accuracy on the dev data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans3) The accuracy on the dev data is 93.56%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on dev data: 93.56%\n"
     ]
    }
   ],
   "source": [
    "# Load the HMM model parameters from hmm.json\n",
    "with open('hmm.json', 'r') as f:\n",
    "    hmm_model = json.load(f)\n",
    "\n",
    "# Extract transition and emission probabilities\n",
    "transition_probabilities = hmm_model['transition']\n",
    "emission_probabilities = hmm_model['emission']\n",
    "state_list = state_list\n",
    "vocab = vocab\n",
    "\n",
    "# Define a function for greedy decoding\n",
    "def greedy_decode(sentence, state_list, transition_probabilities, emission_probabilities, vocab):\n",
    "    tags = []\n",
    "    prev_tag = 'Initial_state'\n",
    "\n",
    "    for token in sentence:\n",
    "        word = token\n",
    "\n",
    "        # Preprocess the word (e.g., handle numbers and unknown words)\n",
    "        if word.isnumeric():\n",
    "            word = '<num>'\n",
    "        if word not in vocab:\n",
    "            word = '<unk>'\n",
    "\n",
    "        # Initialize the best tag and best score\n",
    "        best_tag = None\n",
    "        best_score = 0\n",
    "\n",
    "        for tag in state_list:\n",
    "            if tag != 'Initial_state':  # Exclude the 'START' tag\n",
    "                # Calculate the score for the current tag\n",
    "                score = transition_probabilities.get(str((prev_tag, tag)), 1e-6) * emission_probabilities.get(str((tag, word)), 0)\n",
    "\n",
    "                # Update the best tag if the score is higher\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_tag = tag\n",
    "\n",
    "        tags.append(best_tag)\n",
    "        prev_tag = best_tag\n",
    "\n",
    "    return tags\n",
    "\n",
    "# Load the development data from the JSON file\n",
    "with open('data/dev.json', 'r') as f:\n",
    "    dev_data = json.load(f)\n",
    "    \n",
    "# Initialize variables for accuracy calculation\n",
    "total_tokens = 0\n",
    "correct_tokens = 0\n",
    "\n",
    "# Process and decode each sentence in the development data\n",
    "\n",
    "for item in dev_data:\n",
    "    sentence = item['sentence']\n",
    "    gold_labels = item['labels']\n",
    "    predicted_tags = greedy_decode(sentence, state_list, transition_probabilities, emission_probabilities, vocab)\n",
    "\n",
    "    # Calculate accuracy for this sentence\n",
    "    correct_tokens += sum(1 for gold, pred in zip(gold_labels, predicted_tags) if gold == pred)\n",
    "    total_tokens += len(gold_labels)\n",
    "\n",
    "# Calculate accuracy on the development data\n",
    "accuracy = correct_tokens / total_tokens\n",
    "# Print the accuracy\n",
    "print(f'Accuracy on dev data: {accuracy:.2%}')             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation of the above code:\n",
    "\n",
    "##### Loading HMM Model: The code starts by opening and reading a JSON file named 'hmm.json', which contains the previously learned Hidden Markov Model (HMM) parameters, including transition and emission probabilities.\n",
    "\n",
    "##### Extracting Model Parameters: It extracts the transition and emission probabilities from the loaded HMM model and assigns them to the variables transition_probabilities and emission_probabilities. The variables state_list and vocab are also passed to the decoding function.\n",
    "\n",
    "##### Defining the Greedy Decoding Function: A function named greedy_decode is defined. This function performs the part-of-speech tagging using the learned HMM parameters. It takes the input sentence, the list of possible states (state_list), transition and emission probabilities, and the vocabulary (vocab) as arguments.\n",
    "\n",
    "##### Decoding Sentences: The code then loads the development data from the JSON file named 'data/dev.json'. This data likely contains sentences and their corresponding gold labels (correct part-of-speech tags).\n",
    "\n",
    "##### Accuracy Calculation Variables: Two variables, total_tokens and correct_tokens, are initialized to calculate the accuracy. total_tokens will keep track of the total number of tokens (words or labels) processed, and correct_tokens will count how many tokens were tagged correctly.\n",
    "\n",
    "##### Sentence Decoding and Accuracy Calculation: The code iterates through each item in the development data. For each item, it retrieves the sentence and the gold labels. It then uses the greedy_decode function to predict part-of-speech tags for the sentence based on the learned HMM model.\n",
    "\n",
    "##### Calculating Accuracy for Each Sentence: For each sentence, the code calculates how many tokens were correctly tagged by comparing the predicted tags with the gold labels. It increments the correct_tokens variable accordingly.\n",
    "\n",
    "##### Final Accuracy Calculation: After processing all sentences, the code calculates the accuracy by dividing the number of correct tokens (correct_tokens) by the total number of tokens in the development data (total_tokens). It prints the accuracy as a percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 4: Viterbi Decoding with HMM (30 Points)\n",
    "##### The fourth task is to implement the viterbi decoding algorithm with HMM.\n",
    "##### Task. Implementing the viterbi decoding algorithm and evaluate it on the development data. Predicting the part-of-speech tags of the sentences in the test data and output the predictions in a file named viterbi.json, in the same format of training data.\n",
    "##### Additionally, kindly provide answers to the following questions:\n",
    "##### What is the accuracy on the dev data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans4) The accuracy of viterbi decoding on the dev set is 94.83%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5527/5527 [03:47<00:00, 24.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on dev data: 94.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "# Load the HMM model parameters from hmm.json\n",
    "with open('hmm.json', 'r') as f:\n",
    "    hmm_model = json.load(f)\n",
    "\n",
    "# Extract transition and emission probabilities\n",
    "transition_probabilities = hmm_model['transition']\n",
    "emission_probabilities = hmm_model['emission']\n",
    "\n",
    "vocab = vocab\n",
    "state_list = state_list\n",
    "\n",
    "def viterbi_decode(sentence, state_list, transition_probabilities, emission_probabilities, vocab):\n",
    "    viterbi = np.zeros((len(state_list), len(sentence)), dtype=np.longdouble)\n",
    "    tag = np.zeros((len(state_list), len(sentence)), dtype=np.int32)\n",
    "    predicted_tag = [None] * len(sentence)\n",
    "    \n",
    "    for index, word in enumerate(sentence):\n",
    "        if word.isnumeric():\n",
    "            word = '<num>'\n",
    "        if word not in vocab.keys():\n",
    "            word = '<unk>'\n",
    "        if index == 0:\n",
    "            break\n",
    "    \n",
    "    for i, label in enumerate(state_list):\n",
    "        viterbi[i, 0] = transition_probabilities.get(str(('Initial_state', label)), 1e-6) * emission_probabilities.get(str((label, word)), 0)\n",
    "        tag[i, 0] = -1\n",
    "    \n",
    "    for index in range(1, len(sentence)):\n",
    "        token = sentence[index]\n",
    "        word = token\n",
    "        if word.isnumeric():\n",
    "            word = '<num>'\n",
    "        if word not in vocab.keys():\n",
    "            word = '<unk>'\n",
    "        \n",
    "        for right, state_right in enumerate(state_list):\n",
    "            best_score = 0\n",
    "            max_k = 0\n",
    "            for left, state_left in enumerate(state_list):\n",
    "                score = viterbi[left, index-1] * transition_probabilities.get(str((state_left, state_right)), 1e-6)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    max_k = left\n",
    "            viterbi[right, index] = best_score * emission_probabilities.get(str((state_right, word)), 0)\n",
    "            tag[right, index] = max_k\n",
    "    \n",
    "    predicted_tag[-1] = state_list[np.argmax(viterbi[:,-1])]\n",
    "    \n",
    "    for i in range(len(sentence) - 2, -1, -1):\n",
    "        predicted_tag[i] = state_list[int(tag[state_list.index(predicted_tag[i + 1]), i+1])]\n",
    "    \n",
    "    return predicted_tag\n",
    "\n",
    "# Load the development data from the JSON file\n",
    "with open('data/dev.json', 'r') as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "# Initialize variables for accuracy calculation\n",
    "total_tokens = 0\n",
    "correct_tokens = 0\n",
    "\n",
    "for item in tqdm.tqdm(dev_data):\n",
    "    index = item['index']\n",
    "    sentence = item['sentence']\n",
    "    gold_labels = item['labels']\n",
    "    predicted_tags = viterbi_decode(sentence, state_list, transition_probabilities, emission_probabilities, vocab)\n",
    "\n",
    "    # Calculate accuracy for this sentence\n",
    "    correct_tokens += sum(1 for gold, pred in zip(gold_labels, predicted_tags) if gold == pred)\n",
    "    total_tokens += len(gold_labels)\n",
    "\n",
    "# Calculate accuracy on the development data\n",
    "accuracy = correct_tokens / total_tokens\n",
    "# Print the accuracy\n",
    "print(f'Accuracy on dev data: {accuracy:.2%}')             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation of the above code:\n",
    "\n",
    "##### Loading HMM Model: The code starts by opening and reading a JSON file named 'hmm.json', which contains the previously learned Hidden Markov Model (HMM) parameters, including transition and emission probabilities.\n",
    "\n",
    "##### Extracting Model Parameters: It extracts the transition and emission probabilities from the loaded HMM model and assigns them to the variables transition_probabilities and emission_probabilities. The variables vocab and state_list are also passed to the decoding function.\n",
    "\n",
    "##### Defining the Viterbi Decoding Function: A function named viterbi_decode is defined. This function performs part-of-speech tagging using the Viterbi algorithm based on the learned HMM parameters. It takes the input sentence, the list of possible states (state_list), transition and emission probabilities, and the vocabulary (vocab) as arguments.\n",
    "\n",
    "##### Viterbi Decoding: Inside the viterbi_decode function, a Viterbi matrix (viterbi) and a tag matrix (tag) are initialized to store intermediate results. A list named predicted_tag is initialized to store the predicted part-of-speech tags for each token in the sentence.\n",
    "\n",
    "##### Preprocessing First Token: The code preprocesses the first token in the sentence by checking if it's numeric or not in the same way as earlier code. This is done only once for efficiency.\n",
    "\n",
    "##### Initialization Step: The Viterbi matrix's first column is initialized based on the transition probabilities from the 'Initial_state' to each state and the emission probability of the first word.\n",
    "\n",
    "##### Dynamic Programming Step: The code iterates through the remaining columns of the Viterbi matrix, filling in each cell with the highest probability score. It uses dynamic programming to compute the scores efficiently.\n",
    "\n",
    "##### Backtracking for Predicted Tags: After computing the Viterbi matrix, the code performs backtracking to determine the most likely sequence of part-of-speech tags (the predicted_tags). Starting from the last token, it traces back to find the most likely tag for each token in the sentence.\n",
    "\n",
    "##### Accuracy Calculation: The code loads the development data and, for each sentence, predicts part-of-speech tags using Viterbi decoding. It then calculates how many of these predictions match the gold labels (correct_tokens) and the total number of tokens processed (total_tokens).\n",
    "\n",
    "##### Final Accuracy: After processing all sentences, the code computes the accuracy by dividing the number of correct tokens by the total number of tokens and expresses it as a percentage. This accuracy score indicates the model's performance on the development data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the below code cells we just iterate through the test json and create the greedy and viterbi json files using the above functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "decoded_data_greedy = []\n",
    "\n",
    "for item in test_data:\n",
    "    index = item['index']\n",
    "    sentence = item['sentence']\n",
    "\n",
    "    predicted_tags = greedy_decode(sentence, state_list, transition_probabilities, emission_probabilities, vocab)\n",
    "    decoded_data_greedy.append({'index': item['index'], 'sentence': sentence, 'labels': predicted_tags})\n",
    "\n",
    "# Output the predictions to greedy.json\n",
    "with open('greedy.json', 'w') as f:\n",
    "    json.dump(decoded_data_greedy, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5462/5462 [03:40<00:00, 24.75it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "decoded_data_viterbi = []\n",
    "\n",
    "for item in tqdm.tqdm(test_data):\n",
    "    index = item['index']\n",
    "    sentence = item['sentence']\n",
    "\n",
    "    predicted_tags = viterbi_decode(sentence, state_list, transition_probabilities, emission_probabilities, vocab)\n",
    "    decoded_data_viterbi.append({'index': item['index'], 'sentence': sentence, 'labels': predicted_tags})\n",
    "\n",
    "# Output the predictions to greedy.json\n",
    "with open('viterbi.json', 'w') as f:\n",
    "    json.dump(decoded_data_viterbi, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
